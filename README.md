# Textual-Data-Literc
Data literacy for textual analysis

## Data Litercy 

Digital or data literacy can be defined as a way to think critically about data, or numbers issued by analysis. 

Textual data literacy is the ability to transform automatically big quantities of text into numerical data through applied statistical methods.

The ACRL (Association of College Research Libraries) already in 2015 advised to go beyond information literacy, incorporating also data information literacy to courses addressed to students. 

Moreover nowadays, "Textual retrieval", or the ability to retrieve information from information extracted from texts, is considered relevant, to inferr from textual data analysis trends and patterns about contents.

## Data in general and in the programming language 

Broadly speaking data can be quantitatives or qualitatives, or in other words numbers or texts (unstructured data), howether if we consider the programming language, there are more kind of data, or "data types". 
For instance numbers are "intergers", if whole numbers, or "floats" if they are measured in fractions. 
Text, or textual data can be called "strings", as they consist in letters (for a computer simply signs).

In a "flow control", or the line of commands asking the computer to perform something, the result of the application of boolean operators (AND, OR, NOT) produces two other data types: TRUE or FALSE.

A "list" is a way to store different data types, possibly million of items in a ordered way, and a "data structure" is how information can be stored inside a program.

## Files and files formats

Some kind of data file formats are:

Tabular data, or data organized in raws and colums, CSV files with the (.csv) extension, or Excel files (former proprietory, now an open file format) with the extension .(.xlsx).

Unstructured data, textual data, or text files with the extension (.txt) with only text and no formatting (a word file is a formatted one), Portable Document file with the extension (.pdf),
Images, of images contained in files, or JPG with the extension (.jpg), or (.png)\n", other files used for the data interchange: Markup HyperText Markup Language (.html).

## Why data curation, or "know your data"

Data curation is the activity done on data in order to analyze and store them properly, as for instance: cleaning, processing, munging, archiving, re-use, communicate and share. To perform all those activities in necessary to know where to store data for long term preservation, how to name files (to give metadata), be careful about proprietary formats and their evolution on time, be attentive on how to share data for pourposes of reproducibility .
In the case of textual data the data curation is called "pre-processing" and it aims is to tranform raw data into data ready to be analyzed.

## Computational literacy

It can be a synonim of data litearcy, but computational literacy consider not only data, but software using data, or alternatively customized ways to create "command line" to do analysis autonomously. 

In order to deal with data is necessary some computational literacy, or as Professor Passarotti (Passarotti, 2022) says a "computational" or "automatic" way to think enabling to foresee what it can be done with automation.
Computational literacy is the ability to use automatic analysis devices, knowing how they work and what one can expect from, in order of being able to interpret results duly.

 
## Python literacy, or the "Pythonic way"

A piece of this literacy is all the instructions enabling people to use Phyton as an "empowering" instrument to do customized text analysis. Phyton being a programming language, learning to write codes is equivalent to learn a foreign language.
"
"\n",
 print "Hello word"\n",

  < "" 
   print "Hello word"
   
   "">


## Large Language Models Literacy

AI applications have spread in different domains as for instance in scientific literature searching, this is entailing a new way to do bibliographic search ("systematic literature reviews") by exploring topics (Solveing et al., 2023). The problem is that most of the time services proposing systematic literature reviews are lacking in methodology transparency, frequently algorithms used to do systematic literature review are not open, but proprietary, and in many cases one does not know how the algorithms work and even where the search has been done (on which collection of texts). Huge are the consequences in terms of research accountability and reproducibility.

Because of the recent spread of AI applications, also an AI literacy has become necessary, because knowing how language models works is necessary to understand how their applications propose results and what one can realistically expect from. A piece of  literacy is to let people known that beside proprietary language models there are also open source ones.

For instance Hugging face is a repository of open source language models that can be consulted to have an idea. Web source: https://huggingface.co/

## API literacy

Definition of APIs and their use nowadays.

## RAG, Retrieval Augmented Generation pipeline

RAG literacy is useful to make LLMs more performant and to avoid hallucinations. In the pipeline the model is trained on specific data (a customized knowledge base) as to use them to answer to the question and not the commonly webscraped source of information. More accurate information used enables to be more precise in the answering.


## Information Retrieval on a set of documents hosted in a database is not Information Extraction on an assembled corpus

Doing Information Retrieval on a database one can have an idea about contents, one can use meta-data to filter information, but still one has only some hints on a delimited contents. It is a way to do literature search on specific database, but not a systematic literature review on a topic.

An alternative is to build one's own corpus and uphold it to the platform. Two choices are possible: formatting data as the one in Jstor Archive to use Constellate Jupyter notebooks to do the analysis, or use the Constellate Lab to write down your won personal notebooks to do the analysis.

## Why creating your own corpus?

Assembling a corpus is already doing research, first looking for all sources and documents at disposal and then doing Information Extraction on the corpus. As sources of information can be used: databases' contents (bibliographic references and full-texts), pdf to be transformed in text files by OCR, texts in the public domain contained in platforms (Project Gutemberg), web-scraping from internet.

No quantitative analysis would be sufficient without inquiring the nature of the corpus (J. D. Porter, TAP Institute 2023), all calculations and measures run on the corpus has to be confronted with how the corpus has been built (completeness, or missing contents, capacity to represent or biases). This is valuable especially for Digital Humanities, but not only. Or in other words when doing research \"one has to his own data\" (Passarotti, 2022).

## The importance of keep on learning

Automatic Text Analysis Course (to be graually delivered on BlackBoard

## References

    "1. <a id=\"2\"></a><a id=\"2\"></a> ACRL (Association of College and Research Libraries) «Framework for Information Literacy for Higher Education», Chicago, American Library Association, 2015  \n",
    "2.<a id=\"2\"></a><a id=\"2\"></a> Bauder, Julia (a cura di) “Data Literacy in Academic Libraries. Teaching Critical Thinking with Numbers”, Chicago, ALA (American Library Association) Editions, 2021\n",
    "3. <a id=\"2\"></a> Ciotti Fabio (edited by) “Digital Humanities. Metodi, strumenti, saperi”, Collana Studi superiori, n. 1376, Teoria della letteratura e critica letteraria Carocci editore, Roma, 2023Ciotti Fabio (edited by) “Digital Humanities. Metodi, strumenti, saperi”, Collana Studi superiori, n. 1376, Teoria della letteratura e critica letteraria Carocci editore, Roma, 2023\n",
    "4. <a id=\"2\"></a> 4. <a id=\"2\"></a> Constellate LibGuide by Biblioteca, Università Cattolica del Sacro Cuore di Milano: https://unicatt.libguides.com/constellate/eng\n",
    "5. <a id=\"2\"></a> Constellate website: https://www-constellate-org.ezproxy.unicatt.it \n",
    "6. <a id=\"2\"></a> Passarotti, Marco “Computational Thinking. Dall’automazione alla svolta computazionale nelle scienze umanistiche”, slides presentate il 6. maggio 2022, al convegno «Humanism and Digitization. Theory and Practical Achievements», at Università Cattolica del Sacro Cuore, Milano, at disposal at page: https://zenodo.org/record/6517625#.Yxr973ZByUk\n",
    "7. <a id=\"2\"></a> Vils Anne, Lorna Widgaard, Solveig Sandal Johnsen \"AI-powered software for literature searching: potential for libraries\", June 2023, slides at disposal at page: https://www.inconecss.eu/wp-content/uploads/2023/06/23-06-12-johnsen.pdf"

For any question please write an email to: deborah.grbac@unicatt.it.



